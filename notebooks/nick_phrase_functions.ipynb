{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "M6PQEK4eZyj7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Google drive\n"
      ],
      "metadata": {
        "id": "vSVfBgb9Z7V4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IijnM1bIeUPR",
        "outputId": "373f8ff9-c2b4-4ec2-86f1-b3a0788f858d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/Colab Notebooks/ITC/Final Project/data.npy'\n",
        "data = np.load(path, allow_pickle = True)"
      ],
      "metadata": {
        "id": "SOUVfkCLeGX9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(data, columns = ['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n",
        "       'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text'])"
      ],
      "metadata": {
        "id": "hKvBtxT2EwA3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Keep only 3 features"
      ],
      "metadata": {
        "id": "mT-JFMa3SbsB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "h4aLUdzjSKaQ"
      },
      "outputs": [],
      "source": [
        "df = df.loc[:,['ProductId','Text','Score']]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['Text'] = df['Text'].str.replace(r'<[^>]*>', '', regex=True)"
      ],
      "metadata": {
        "id": "d_lxF4ar_Sns"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Split the dataset into train val and test using 100k samples"
      ],
      "metadata": {
        "id": "kzj1VlMFTmjO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "INm9_ZCgH1Na"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "splitter_temp = GroupShuffleSplit(test_size=.40, n_splits=1, random_state = 42)\n",
        "split_temp = splitter_temp.split(df[:100000], groups=df[:100000]['ProductId'])\n",
        "train_inds, temp_inds = next(split_temp)\n",
        "\n",
        "train = df.iloc[train_inds]\n",
        "temp = df.iloc[temp_inds]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zg059kSjDOwX"
      },
      "outputs": [],
      "source": [
        "splitter_val = GroupShuffleSplit(test_size=.50, n_splits=1, random_state = 42)\n",
        "split_val = splitter_val.split(temp, groups=temp['ProductId'])\n",
        "val_inds, test_inds = next(split_val)\n",
        "\n",
        "val = temp.iloc[val_inds]\n",
        "test = temp.iloc[test_inds]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "c8KrlV2SJ_Tv"
      },
      "outputs": [],
      "source": [
        "X_train = train.drop(columns = 'Score')\n",
        "y_train = train.Score\n",
        "\n",
        "X_val = val.drop(columns = 'Score')\n",
        "y_val = val.Score\n",
        "\n",
        "X_test = test.drop(columns = 'Score')\n",
        "y_test = test.Score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Full Pipeline"
      ],
      "metadata": {
        "id": "jDXy4urYV-t-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "jGHJa1-kEIAw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define number of samples\n",
        "nbr_sent = 1000\n",
        "\n",
        "# Define subject-pronouns to remove\n",
        "personal_pronouns = ['i', 'me', 'you', 'he', 'she', 'it', 'we', 'us', 'they', 'them']"
      ],
      "metadata": {
        "id": "YcTh7LeiWdmV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Wave 1*"
      ],
      "metadata": {
        "id": "MH-BJHmzWHLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Finds root of sentence (usually verb) and returns phrases based on children\n",
        "wave_1 = []\n",
        "score_noun = []\n",
        "indexes_noun = []\n",
        "adv_verb_ind = [] # tackle the case 'delivery was REALLY NOT quick'\n",
        "for i in tqdm(range(nbr_sent)):\n",
        "  sente = nlp(X_train.Text.iloc[i])\n",
        "  for token in sente:\n",
        "    noun = ''\n",
        "    adj = ''\n",
        "    adverb = ''\n",
        "    neg = ''\n",
        "    adv_verb = ''\n",
        "    if token.dep_ == 'ROOT':\n",
        "      for child in token.children:\n",
        "        if child.pos_ == 'NOUN':\n",
        "          noun = child.text\n",
        "        elif child.pos_ == 'ADJ':\n",
        "          adj = child.text\n",
        "          for other_child in child.children:\n",
        "            if other_child.pos_ == 'ADV':\n",
        "              adverb = other_child.text\n",
        "        elif child.pos_ == 'ADV':\n",
        "          adv_verb_ind.append((i,child.text))\n",
        "          adv_verb = child.text\n",
        "        elif child.pos_ == 'PART':\n",
        "          neg = child.text\n",
        "\n",
        "\n",
        "      if noun and adj:\n",
        "        indexes_noun.append(i)\n",
        "        score_noun.append(y_train.iloc[i])\n",
        "        if adverb :\n",
        "          wave_1.append((noun, token.text,adverb,adj))\n",
        "        elif adv_verb and neg:\n",
        "          wave_1.append((noun, token.text,adv_verb,neg,adj))\n",
        "        elif neg:\n",
        "          wave_1.append((noun, token.text,neg,adj))\n",
        "        else:\n",
        "          wave_1.append((noun, token.text,adj))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3g622e13-tgC",
        "outputId": "203edb12-c04a-4058-952e-35ede086149d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:21<00:00, 46.76it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Percent of Phrases Captured:{(len(wave_1)/nbr_sent*100): .2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfL6vekI6hvd",
        "outputId": "94e1595b-52e2-479f-a5a7-d3d2a1c2cd81"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percent of Phrases Captured: 48.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Wave 2*"
      ],
      "metadata": {
        "id": "c4eopBExWMzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns phrases using passive voice\n",
        "wave_2 = []\n",
        "\n",
        "for i in tqdm(range(nbr_sent)):\n",
        "  sente = nlp(X_train.Text.iloc[i])\n",
        "  for token in sente:\n",
        "    # Check if the token is a verb\n",
        "    if token.dep_ == 'ROOT':\n",
        "        subject = ''\n",
        "        dobj_text = ''\n",
        "        adj_text = ''\n",
        "\n",
        "        # Look for the subject of the verb\n",
        "        for child in token.children:\n",
        "            if child.dep_ in ['nsubj', 'nsubjpass'] and child.text.lower() not in personal_pronouns:  # nsubjpass for passive voice\n",
        "                subject = child.text\n",
        "            # Check if there is a direct object that is a noun\n",
        "            elif child.dep_ == 'dobj' and child.pos_ == 'NOUN':\n",
        "                dobj_text = child.text\n",
        "                # Iterate over the children of the noun to find an adjective\n",
        "                for grandchild in child.children:\n",
        "                    if grandchild.dep_ == 'amod' and grandchild.pos_ == 'ADJ':\n",
        "                        adj_text = grandchild.text\n",
        "\n",
        "        if subject and dobj_text and adj_text:\n",
        "          wave_2.append((subject, token.text, adj_text, dobj_text))\n",
        "            # wave_2.append(f\"{subject} {token.text} {adj_text} {dobj_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Md-MgSKsWRK2",
        "outputId": "76b745e0-3e4c-4006-f80d-7edf376fb7cd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:19<00:00, 50.24it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_phrases = wave_1 + wave_2\n",
        "print(f\"Percent of Phrases Captured:{(len(final_phrases)/nbr_sent*100): .2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhqJMVw5XPDt",
        "outputId": "57490216-9b38-4a36-d3b6-2f0904644daa"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percent of Phrases Captured: 58.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Top nouns ready for sentiment analysis\n",
        "noun_counts = Counter(nlp(item[0])[0].lemma_ for item in final_phrases)\n",
        "\n",
        "sorted_nouns = sorted(noun_counts.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "for noun, count in sorted_nouns[:10]:\n",
        "  print(f\"{noun}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sg1DNV9eYS8_",
        "outputId": "000e18d6-0460-4a05-8607-be600cdfd7fc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chip: 62\n",
            "flavor: 29\n",
            "taste: 18\n",
            "bag: 16\n",
            "price: 15\n",
            "product: 15\n",
            "coffee: 11\n",
            "tea: 10\n",
            "potato: 10\n",
            "food: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "1s4i-W2KEQs9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sub_phrases(X, nbr_sent):\n",
        "  \"\"\"Returns lists of tuple phrases\"\"\"\n",
        "  wave_1 = []\n",
        "  wave_2 = []\n",
        "\n",
        "  for i in tqdm(range(nbr_sent)):\n",
        "    sente = nlp(X.Text.iloc[i])\n",
        "    for token in sente:\n",
        "      noun = ''\n",
        "      adj = ''\n",
        "      adverb = ''\n",
        "      neg = ''\n",
        "      adv_verb = ''\n",
        "      subject = ''\n",
        "      dobj_text = ''\n",
        "      adj_text = ''\n",
        "      if token.dep_ == 'ROOT':\n",
        "        for child in token.children:\n",
        "          if child.pos_ == 'NOUN':\n",
        "            noun = child.text\n",
        "          elif child.pos_ == 'ADJ':\n",
        "            adj = child.text\n",
        "            for other_child in child.children:\n",
        "              if other_child.pos_ == 'ADV':\n",
        "                adverb = other_child.text\n",
        "          elif child.pos_ == 'ADV':\n",
        "            adv_verb_ind.append((i,child.text))\n",
        "            adv_verb = child.text\n",
        "          elif child.pos_ == 'PART':\n",
        "            neg = child.text\n",
        "\n",
        "          if child.dep_ in ['nsubj', 'nsubjpass'] and child.text.lower() not in personal_pronouns:  # nsubjpass for passive voice\n",
        "            subject = child.text\n",
        "\n",
        "          elif child.dep_ == 'dobj' and child.pos_ == 'NOUN':\n",
        "            dobj_text = child.text\n",
        "            for grandchild in child.children:\n",
        "              if grandchild.dep_ == 'amod' and grandchild.pos_ == 'ADJ':\n",
        "                  adj_text = grandchild.text\n",
        "\n",
        "        if noun and adj:\n",
        "          if adverb :\n",
        "            wave_1.append((noun, token.text,adverb,adj))\n",
        "          elif adv_verb and neg:\n",
        "            wave_1.append((noun, token.text,adv_verb,neg,adj))\n",
        "          elif neg:\n",
        "            wave_1.append((noun, token.text,neg,adj))\n",
        "          else:\n",
        "            wave_1.append((noun, token.text,adj))\n",
        "\n",
        "        if subject and dobj_text and adj_text:\n",
        "          wave_2.append((subject, token.text, adj_text, dobj_text))\n",
        "\n",
        "  return wave_1 + wave_2\n",
        "\n",
        "\n",
        "def get_topics(phrases, num_topics):\n",
        "  \"\"\"Returns dictionary of topics with sorted by count\"\"\"\n",
        "  noun_counts = Counter(nlp(item[0])[0].lemma_ for item in phrases)\n",
        "  sorted_nouns = sorted(noun_counts.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "  for noun, count in sorted_nouns[:num_topics]:\n",
        "    print(f\"{noun}: {count}\")\n",
        "\n",
        "def print_phrases(phrases, num_phrases):\n",
        "  for phrase in phrases[:num_phrases]:\n",
        "    print(phrase)"
      ],
      "metadata": {
        "id": "RwoGz3OwETZ1"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constructs sub phrases\n",
        "sub_phrases = create_sub_phrases(X=X_train, nbr_sent=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBa0_cz1Ew1I",
        "outputId": "db2b5977-bb0b-4b44-b259-87c2832f3bc6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:19<00:00, 50.99it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The data contains {len(sub_phrases)} phrases\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPB12bLPHn9F",
        "outputId": "0cc09bd5-d1c1-4444-996f-704d78429ff2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The data contains 581 phrases\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Constructs topics\n",
        "topics = get_topics(phrases=sub_phrases, num_topics=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zp1zcPoULZWu",
        "outputId": "00ee4173-05c0-400c-9ea7-eb60c3274fc1"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chip: 62\n",
            "flavor: 29\n",
            "taste: 18\n",
            "bag: 16\n",
            "price: 15\n",
            "product: 15\n",
            "coffee: 11\n",
            "tea: 10\n",
            "potato: 10\n",
            "food: 9\n",
            "stuff: 7\n",
            "one: 7\n",
            "size: 6\n",
            "amount: 6\n",
            "brand: 5\n",
            "pack: 5\n",
            "package: 5\n",
            "texture: 5\n",
            "son: 5\n",
            "licorice: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B9zEWjIgNyuo"
      },
      "execution_count": 26,
      "outputs": []
    }
  ]
}